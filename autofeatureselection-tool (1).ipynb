{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# We dont Probably need the Gridlines. Do we? If yes comment this line\nsns.set(style=\"ticks\")\n\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nflatui = sns.color_palette(flatui)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-05-21T15:33:52.510201Z","iopub.execute_input":"2022-05-21T15:33:52.510496Z","iopub.status.idle":"2022-05-21T15:33:52.524697Z","shell.execute_reply.started":"2022-05-21T15:33:52.510462Z","shell.execute_reply":"2022-05-21T15:33:52.523973Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# AutoFeatureSelector Tool\n\n### The task of this notebook is to revise our understanding of various Feature Selection methods the ability to apply this knowledge in a real-world dataset to select best features and also to build an automated feature selection tool as a toolkit\n\n### The following methods are applied:\n\n- Pearson Correlation\n- Chi-Square\n- RFE\n- Embedded\n- Tree (Random Forest)\n- Tree (Light GBM)","metadata":{}},{"cell_type":"markdown","source":"Dataset: FIFA 19 Player Skills\nAttributes: FIFA 2019 players attributes like Age, Nationality, Overall, Potential, Club, Value, Wage, Preferred Foot, International Reputation, Weak Foot, Skill Moves, Work Rate, Position, Jersey Number, Joined, Loaned From, Contract Valid Until, Height, Weight, LS, ST, RS, LW, LF, CF, RF, RW, LAM, CAM, RAM, LM, LCM, CM, RCM, RM, LWB, LDM, CDM, RDM, RWB, LB, LCB, CB, RCB, RB, Crossing, Finishing, Heading, Accuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, and Release Clause.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\nimport scipy.stats as ss\nfrom collections import Counter\nimport math \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:52.545757Z","iopub.execute_input":"2022-05-21T15:33:52.546489Z","iopub.status.idle":"2022-05-21T15:33:52.553352Z","shell.execute_reply.started":"2022-05-21T15:33:52.546435Z","shell.execute_reply":"2022-05-21T15:33:52.552549Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Reading the Data and Preprocessing","metadata":{}},{"cell_type":"code","source":"player_df = pd.read_csv(\"../input/fifa-19-player-dataset/FIFA19.csv\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-05-21T15:33:52.575306Z","iopub.execute_input":"2022-05-21T15:33:52.575850Z","iopub.status.idle":"2022-05-21T15:33:52.826520Z","shell.execute_reply.started":"2022-05-21T15:33:52.575807Z","shell.execute_reply":"2022-05-21T15:33:52.825353Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"player_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:52.828736Z","iopub.execute_input":"2022-05-21T15:33:52.828989Z","iopub.status.idle":"2022-05-21T15:33:52.879601Z","shell.execute_reply.started":"2022-05-21T15:33:52.828960Z","shell.execute_reply":"2022-05-21T15:33:52.878670Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"numcols = ['Overall', 'Crossing','Finishing',  'ShortPassing',  'Dribbling','LongPassing', 'BallControl', 'Acceleration','SprintSpeed', 'Agility',  'Stamina','Volleys','FKAccuracy','Reactions','Balance','ShotPower','Strength','LongShots','Aggression','Interceptions']\ncatcols = ['Preferred Foot','Position','Nationality','Weak Foot']","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:52.880876Z","iopub.execute_input":"2022-05-21T15:33:52.881201Z","iopub.status.idle":"2022-05-21T15:33:52.887102Z","shell.execute_reply.started":"2022-05-21T15:33:52.881169Z","shell.execute_reply":"2022-05-21T15:33:52.886314Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"player_df = player_df[numcols+catcols]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:52.888924Z","iopub.execute_input":"2022-05-21T15:33:52.889393Z","iopub.status.idle":"2022-05-21T15:33:52.907861Z","shell.execute_reply.started":"2022-05-21T15:33:52.889349Z","shell.execute_reply":"2022-05-21T15:33:52.906838Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"traindf = pd.concat([player_df[numcols], pd.get_dummies(player_df[catcols])],axis=1)\nfeatures = traindf.columns\n\ntraindf = traindf.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:52.909727Z","iopub.execute_input":"2022-05-21T15:33:52.910328Z","iopub.status.idle":"2022-05-21T15:33:53.012434Z","shell.execute_reply.started":"2022-05-21T15:33:52.910275Z","shell.execute_reply":"2022-05-21T15:33:53.011390Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"traindf = pd.DataFrame(traindf,columns=features)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.013983Z","iopub.execute_input":"2022-05-21T15:33:53.014216Z","iopub.status.idle":"2022-05-21T15:33:53.018785Z","shell.execute_reply.started":"2022-05-21T15:33:53.014189Z","shell.execute_reply":"2022-05-21T15:33:53.017602Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y = traindf['Overall']>=87\nX = traindf.copy()\ndel X['Overall']","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.020984Z","iopub.execute_input":"2022-05-21T15:33:53.021520Z","iopub.status.idle":"2022-05-21T15:33:53.040412Z","shell.execute_reply.started":"2022-05-21T15:33:53.021488Z","shell.execute_reply":"2022-05-21T15:33:53.039547Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.042199Z","iopub.execute_input":"2022-05-21T15:33:53.043562Z","iopub.status.idle":"2022-05-21T15:33:53.081995Z","shell.execute_reply.started":"2022-05-21T15:33:53.043507Z","shell.execute_reply":"2022-05-21T15:33:53.081018Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"len(X.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.083471Z","iopub.execute_input":"2022-05-21T15:33:53.083854Z","iopub.status.idle":"2022-05-21T15:33:53.091527Z","shell.execute_reply.started":"2022-05-21T15:33:53.083807Z","shell.execute_reply":"2022-05-21T15:33:53.090618Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Set some Parameters","metadata":{}},{"cell_type":"code","source":"feature_name = list(X.columns)\n# no of maximum features we need to select\nnum_feats=30","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.095777Z","iopub.execute_input":"2022-05-21T15:33:53.096049Z","iopub.status.idle":"2022-05-21T15:33:53.104081Z","shell.execute_reply.started":"2022-05-21T15:33:53.096020Z","shell.execute_reply":"2022-05-21T15:33:53.103364Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 1. Pearson correlation","metadata":{}},{"cell_type":"markdown","source":"This is a filter-based method. \nWe check the absolute value of the Pearson's correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.","metadata":{}},{"cell_type":"code","source":"def cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.105185Z","iopub.execute_input":"2022-05-21T15:33:53.105771Z","iopub.status.idle":"2022-05-21T15:33:53.218226Z","shell.execute_reply.started":"2022-05-21T15:33:53.105735Z","shell.execute_reply":"2022-05-21T15:33:53.216880Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"cor_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.220035Z","iopub.execute_input":"2022-05-21T15:33:53.220370Z","iopub.status.idle":"2022-05-21T15:33:53.228499Z","shell.execute_reply.started":"2022-05-21T15:33:53.220326Z","shell.execute_reply":"2022-05-21T15:33:53.227464Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# 2. Chi-Square Features","metadata":{}},{"cell_type":"markdown","source":"This is another filter-based method. \nIn this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.230450Z","iopub.execute_input":"2022-05-21T15:33:53.230762Z","iopub.status.idle":"2022-05-21T15:33:53.769707Z","shell.execute_reply.started":"2022-05-21T15:33:53.230717Z","shell.execute_reply":"2022-05-21T15:33:53.768798Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"chi_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.771323Z","iopub.execute_input":"2022-05-21T15:33:53.772200Z","iopub.status.idle":"2022-05-21T15:33:53.782868Z","shell.execute_reply.started":"2022-05-21T15:33:53.772149Z","shell.execute_reply":"2022-05-21T15:33:53.781612Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# 3. Recursive Feature Elimination","metadata":{}},{"cell_type":"markdown","source":"This is a wrapper based method.\nFrom sklearn Documentation:\n\n>>The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nAs you would have guessed we could use any estimator with the method. In this case, we use LogisticRegression and the RFE observes the coef_ attribute of the LogisticRegression object","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:33:53.784878Z","iopub.execute_input":"2022-05-21T15:33:53.787944Z","iopub.status.idle":"2022-05-21T15:34:00.831977Z","shell.execute_reply.started":"2022-05-21T15:33:53.787862Z","shell.execute_reply":"2022-05-21T15:34:00.831002Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"rfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:00.837994Z","iopub.execute_input":"2022-05-21T15:34:00.840848Z","iopub.status.idle":"2022-05-21T15:34:00.856555Z","shell.execute_reply.started":"2022-05-21T15:34:00.840779Z","shell.execute_reply":"2022-05-21T15:34:00.855452Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"rfe_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:00.862725Z","iopub.execute_input":"2022-05-21T15:34:00.866021Z","iopub.status.idle":"2022-05-21T15:34:00.881458Z","shell.execute_reply.started":"2022-05-21T15:34:00.865947Z","shell.execute_reply":"2022-05-21T15:34:00.880168Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# 4. Lasso: SelectFromModel","metadata":{}},{"cell_type":"markdown","source":"This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods. \nFor example, Lasso, and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero. \nHere we use Lasso to select variables.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:41.950087Z","iopub.execute_input":"2022-05-21T15:34:41.950953Z","iopub.status.idle":"2022-05-21T15:34:42.431556Z","shell.execute_reply.started":"2022-05-21T15:34:41.950910Z","shell.execute_reply":"2022-05-21T15:34:42.430544Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"embeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:48.104200Z","iopub.execute_input":"2022-05-21T15:34:48.104534Z","iopub.status.idle":"2022-05-21T15:34:48.113743Z","shell.execute_reply.started":"2022-05-21T15:34:48.104496Z","shell.execute_reply":"2022-05-21T15:34:48.112721Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"embeded_lr_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:50.926879Z","iopub.execute_input":"2022-05-21T15:34:50.927333Z","iopub.status.idle":"2022-05-21T15:34:50.933616Z","shell.execute_reply.started":"2022-05-21T15:34:50.927298Z","shell.execute_reply":"2022-05-21T15:34:50.932658Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# 5. Tree-based: SelectFromModel","metadata":{}},{"cell_type":"markdown","source":"This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.\nWe can also use RandomForest to select features based on feature importance.\nWe calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\nembeded_rf_selector.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:53.873379Z","iopub.execute_input":"2022-05-21T15:34:53.873877Z","iopub.status.idle":"2022-05-21T15:34:55.765421Z","shell.execute_reply.started":"2022-05-21T15:34:53.873829Z","shell.execute_reply":"2022-05-21T15:34:55.764766Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:57.298420Z","iopub.execute_input":"2022-05-21T15:34:57.299179Z","iopub.status.idle":"2022-05-21T15:34:57.330040Z","shell.execute_reply.started":"2022-05-21T15:34:57.299141Z","shell.execute_reply":"2022-05-21T15:34:57.329209Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"embeded_rf_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:34:59.568731Z","iopub.execute_input":"2022-05-21T15:34:59.569374Z","iopub.status.idle":"2022-05-21T15:34:59.574541Z","shell.execute_reply.started":"2022-05-21T15:34:59.569338Z","shell.execute_reply":"2022-05-21T15:34:59.573912Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:35:01.747377Z","iopub.execute_input":"2022-05-21T15:35:01.747688Z","iopub.status.idle":"2022-05-21T15:35:03.255493Z","shell.execute_reply.started":"2022-05-21T15:35:01.747639Z","shell.execute_reply":"2022-05-21T15:35:03.254724Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"embeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:35:05.186200Z","iopub.execute_input":"2022-05-21T15:35:05.186503Z","iopub.status.idle":"2022-05-21T15:35:05.197299Z","shell.execute_reply.started":"2022-05-21T15:35:05.186472Z","shell.execute_reply":"2022-05-21T15:35:05.196717Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"embeded_lgb_feature","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:35:07.345801Z","iopub.execute_input":"2022-05-21T15:35:07.346285Z","iopub.status.idle":"2022-05-21T15:35:07.352032Z","shell.execute_reply.started":"2022-05-21T15:35:07.346242Z","shell.execute_reply":"2022-05-21T15:35:07.351170Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# BONUS","metadata":{}},{"cell_type":"code","source":"\npd.set_option('display.max_rows', None)\n# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:35:14.228369Z","iopub.execute_input":"2022-05-21T15:35:14.228888Z","iopub.status.idle":"2022-05-21T15:35:14.263388Z","shell.execute_reply.started":"2022-05-21T15:35:14.228852Z","shell.execute_reply":"2022-05-21T15:35:14.262598Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}